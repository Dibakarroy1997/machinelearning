{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Data Preprocessing** is done manually.\n",
    "\n",
    "In our case, we:\n",
    "* *We created a dataset/ folder*\n",
    "* *Then created training_set/ and test_set/ subfolders inside dataset/ folder*\n",
    "* *Inside training_set/ and test_set/ subfolders we created cats/ and dogs/ subfolders*\n",
    "* *Then we have placed 4000 cat pictures index 1-4000 in dataset/training_set/cats*\n",
    "* *Then we have placed 4000 dog pictures index 1-4000 in dataset/training_set/dogs*\n",
    "* *Then we have placed 1000 cat pictures index 4001-5000 in dataset/test_set/cats*\n",
    "* *Then we have placed 1000 dog pictures index 4001-5000 in dataset/test_set/dogs*\n",
    "\n",
    "**So now we have 4000 training examples for each class, and 1000 test examples for each class.**\n",
    "\n",
    "Our directory structure is something like this:\n",
    "\n",
    "```\n",
    "dataset/\n",
    "    training_set/\n",
    "        dogs/\n",
    "            dog.1.jpg\n",
    "            dog.2.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat.1.jpg\n",
    "            cat.2.jpg\n",
    "            ...\n",
    "    test_set/\n",
    "        dogs/\n",
    "            dog.4001.jpg\n",
    "            dog.4002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat.4001.jpg\n",
    "            cat.4002.jpg\n",
    "            ...\n",
    "```\n",
    "\n",
    "If you want a bigger dataset then go here: [**[kaggle] Dogs vs. Cats**(Create an algorithm to distinguish dogs from cats)](https://www.kaggle.com/c/dogs-vs-cats/data)\n",
    "\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional Note: *Fitting CNN to the image takes a lot of time and processing power. It would be better if you use tensorflow-gpu. Alternatively you can use any cloud service to train your network on. I personally used FloydHub. There are other alternative too such as AWS, Google Cloud, Paperspace etc. I will link few of them below.***\n",
    "\n",
    "* [AWS](https://aws.amazon.com/)\n",
    "* [Google Cloud](https://cloud.google.com/)\n",
    "* [Paperspace](https://www.paperspace.com/)\n",
    "* [FloydHub](https://www.floydhub.com)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Keras libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(32, (3, 3), input_shape=(64, 64, 3), activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a second convolutional layer followed by pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Full connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.add(Dense(activation=\"relu\", units=128))\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Fitting the CNN to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 139,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 3
      },
      {
       "item_id": 4
      },
      {
       "item_id": 45
      }
     ]
    },
    "colab_type": "code",
    "id": "hNydFKLFLENi",
    "outputId": "e0a6abf2-86fd-41fb-bbfa-1162ed7ca731",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "250/250 [==============================] - 231s - loss: 0.6863 - acc: 0.5567 - val_loss: 0.6840 - val_acc: 0.5528\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 187s - loss: 0.6480 - acc: 0.6283 - val_loss: 0.6220 - val_acc: 0.6745\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 187s - loss: 0.5852 - acc: 0.6909 - val_loss: 0.5571 - val_acc: 0.7184\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 186s - loss: 0.5490 - acc: 0.7192 - val_loss: 0.5198 - val_acc: 0.7564\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 186s - loss: 0.5160 - acc: 0.7424 - val_loss: 0.5173 - val_acc: 0.7384\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 187s - loss: 0.4983 - acc: 0.7530 - val_loss: 0.4937 - val_acc: 0.7512\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 186s - loss: 0.4815 - acc: 0.7645 - val_loss: 0.5162 - val_acc: 0.7542\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 187s - loss: 0.4633 - acc: 0.7740 - val_loss: 0.4748 - val_acc: 0.7775\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 187s - loss: 0.4426 - acc: 0.7883 - val_loss: 0.4785 - val_acc: 0.7754\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 186s - loss: 0.4287 - acc: 0.7985 - val_loss: 0.4653 - val_acc: 0.7910\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 187s - loss: 0.4228 - acc: 0.7988 - val_loss: 0.4506 - val_acc: 0.7892\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 186s - loss: 0.4095 - acc: 0.8139 - val_loss: 0.4646 - val_acc: 0.7879\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 187s - loss: 0.3999 - acc: 0.8183 - val_loss: 0.4876 - val_acc: 0.7860\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 187s - loss: 0.3766 - acc: 0.8283 - val_loss: 0.4700 - val_acc: 0.7891\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 186s - loss: 0.3657 - acc: 0.8390 - val_loss: 0.4514 - val_acc: 0.8093\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 187s - loss: 0.3545 - acc: 0.8425 - val_loss: 0.4668 - val_acc: 0.7999\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 187s - loss: 0.3497 - acc: 0.8442 - val_loss: 0.4651 - val_acc: 0.7965\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 187s - loss: 0.3263 - acc: 0.8598 - val_loss: 0.4673 - val_acc: 0.8044\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 187s - loss: 0.3163 - acc: 0.8635 - val_loss: 0.4659 - val_acc: 0.8103\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 187s - loss: 0.2997 - acc: 0.8707 - val_loss: 0.4745 - val_acc: 0.8096\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 186s - loss: 0.2863 - acc: 0.8751 - val_loss: 0.5066 - val_acc: 0.7871\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 187s - loss: 0.2821 - acc: 0.8746 - val_loss: 0.4680 - val_acc: 0.8089\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 186s - loss: 0.2604 - acc: 0.8895 - val_loss: 0.4885 - val_acc: 0.8130\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 187s - loss: 0.2497 - acc: 0.8962 - val_loss: 0.4733 - val_acc: 0.8087\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 186s - loss: 0.2414 - acc: 0.8988 - val_loss: 0.5130 - val_acc: 0.8076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fae3a700400>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "training_set = train_datagen.flow_from_directory('/my_data/dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('/my_data/dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "classifier.fit_generator(training_set,\n",
    "                         epochs = 25,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 2000,\n",
    "                         steps_per_epoch=250)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Convolutional Neural Networks.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
